---
title: Generating Correlated Data for Omics Simulation
author:
    - name: "Jianing Yang"
      orcid: 0000-0002-2048-9398
      affiliations:
        - name: "Chronobiology and Sleep Institute, University of Pennsylvania"
    - name: "Gregory R. Grant"
      orcid: 0000-0002-0139-7658
      affiliations:
        - name: "Department of Genetics, University of Pennsylvania"
    - name: "Thomas G. Brooks"
      orcid: 0000-0002-6980-0079
      affiliations:
        - name: "Institute for Translational Medicine and Therapeutics, University of Pennsylvania"
editor: 
  markdown: 
    wrap: sentence
execute:
  echo: false
  cache: true
  warning: false
format:
  html:
    toc: true
    toc-expand: true
bibliography: references.bib
---
## Abstract

Simulation of realistic omics data is a key input for benchmarking studies that help users obtain optimal computational pipelines.
Omics data involves large numbers of measured features on each samples and these measures are generally correlated with each other.
However, simulation too often ignores these correlations, perhaps due to the inconvenience and computational hurdles of doing so.
To alleviate this, we describe in detail three approaches for quickly generating omics-scale data with correlated measures which mimic real data sets.
These approaches all are based on a Gaussian copula approach with a covariance matrix that decomposes into a diagonal part and a low-rank part.
We use these approaches to demonstrate the importance of including correlation in two benchmarking applications.
First, we show that variance of results from the popular DESeq2 method increases when dependence is included.
Second, we demonstrate that CYCLOPS, a method for inferring circadian time of collection from transcriptomics, improves in performance when given gene-gene dependencies in some circumstances.
We provide an R package, dependentsimr, that has efficient implementations of these methods and can generate dependent data with arbitrary distributions, including discrete (binary, ordered categorical, Poisson, negative binomial), continuous (normal), or with an empirical distribution.

## Introduction

Omics data typically has far fewer samples than measurements per sample.
This creates dual challenges in generating realistic simulated data for the purposes of benchmarking.
First, there isn’t enough data to be able to compute a dependence structure (e.g., a full-rank correlation matrix).
Second, generating omics-scale data with a specified correlation matrix is slow due to the typical $O(n^3)$ nature of these algorithms, where $n$ is the number of measurements per sample.
Moreover, there is a lack of practical guidance on how to generate simulated data with realistic dependence.
These often mean that simulators assume independence of the measurements, which does not reflect reality.

Here, we give an introduction to the theory and practice of generating dependent data and describe three related solutions which all offer good performance and ease-of-use even for large omics-scale problems.
This expands off a discussion we originally wrote as part of a larger discussion on best practices in omics benchmarking [@Brooks2024].
Our goal here is to produce guidelines that show that generating correlated data does not have to be onerous and instead should be considered a baseline requirement when simulating data.

We present three methods that operate by inferring a covariance matrix that decomposes into a diagonal part and a low-rank part.
Using a Gaussian copula [@Nelsen1998-nq] approach (also referred to as NORTA, for "normal to anything" [@norta]), the marginal (univariate) distributions can have realistic forms.
These solutions operate by taking a real dataset and mimicking it.
For ease of use, we implement this in an R package which supports normal, Poisson, DESeq2-based (negative binomial with sample-specific size factors), and empirical (for ordinal data) marginal distributions.

We implemented three different strategies for determining the diagonal and low-rank parts of the covariance matrix.
First, the 'PCA' method uses principal component analysis (PCA) and picks the low-rank part such that the simulated data has the same variance in the top $k$ PCA components of the reference dataset.
Second, the 'spiked Wishart' method fits $k$ components such that simulations with the same number of samples as the reference dataset will have, on average, the same PCA component variances as the reference.
Unlike 'PCA', these variances are computed with resepect to the simulated data's own PCA and not using the PCA weights of the reference dataset.
Third, the 'corpcor' method uses the popular R library `corpcor` [@SchaferStrimmer2005; @OpgenRheinStrimmer2007], which implements a James-Stein type shrinkage estimator for the covariance matrix as a linear interpolation of the sample covariance matrix and a diagonal matrix.
No method exactly captures the input data, indicating room for future research, but all improve upon the common approach of assuming independence.

We show two applications which demonstrate the effects of including dependence of measurements in simulated data when benchmarking computational pipelines.
First, we simulate RNA-seq data with differential expression between two conditions.
Using DESeq2 to determine the differentially expressed genes, we found that dependence had little impact on the accuracy of reported $p$-values but increased the variance of those estimates.
Second, we simulated a time series of RNA-seq data points and used the CYCLOPS method [@Anafi2017] to infer collection time from the RNA-seq data, without time labels.
Depending upon settings used, performance of CYCLOPS dependent substantially on the dependence structure of the data, and surprisingly showed worst performance when given data with independent genes.

## Results

We start with a reference data set $X$ given by an $p \times n$ data matrix of $p$ features measured in each of $n$ independent samples, which could be any real data set of interest.
Our goal is to generate simulated data set that looks, at least on average, the same as this reference data set.
To do this, we need to capture correlations between the $p$ features, which could represent gene expressions, protein abundances, or other measured values.
We refer to these features as genes for simplicity.

Our methods are based off the well-known Gaussian copula method, which uses a multivariate normal distribution to capture aspects of the gene-gene dependence, without necessarily assuming that the data is normally distributed.
This multivariate normal distribution is parameterized by a $p \times p$ covariance matrix, $\Sigma$.
In omics, choosing the covariance matrix $\Sigma_{sim}$ to use during simulation presents a challenge since $n < p$.
The most obvious choice is to use the sample covariance matrix $\widehat \Sigma$ of the reference data set (possibly after some transformation of the data).
However, using $\widehat \Sigma$ is a poor choice for $\Sigma_{sim}$.
Indeed, we consider a choice of $\Sigma_{sim}$ to be successful if data sets of $n$ samples simulated using $\Sigma_{sim}$ have sample covaraince matrices $\widehat \Sigma_{sim}$ that are similar to $\widehat \Sigma$.
Counter-intuitively, data simulated using $\Sigma_{sim} := \widehat \Sigma$ have sample covariance matrices that systematically and substantially differ from $\widehat \Sigma$, see @fig-intro.

```{r}
#| label: fig-intro
#| fig-cap: "Eigenvalues of the sample covariance matrix $\\widehat \\Sigma$ of reference data compared to $\\widehat \\Sigma_{sim}$ from from simulated data using $\\Sigma_{sim} = \\widehat \\Sigma$. 100 genes and 12 samples were used for both reference and the 20 simulated data sets. Both reference and simulated data follow a multivariate normal distribution. Eigenvalues are numbered from largest to smallest."
#| fig-width: 5
#| fig-height: 4

library(MASS)
library(tidyverse)

# create reference aka 'real' data set
temp <- matrix(rnorm(100*100), 100, 100)
Sigma_true <- temp %*% t(temp)
reference_data <- mvrnorm(n=12, mu=rep(0,100), Sigma = Sigma_true) |> t()
Sigma_hat <- cov(reference_data |> t())


# Run 20 simulations and collate eigenvalues of the sample covariance matrix
temp <- list()
temp[[1]] <-   tibble(
  number = 1:11,
  eigenvalue = eigen(Sigma_hat)$values[1:11],
  type = "reference",
  iter = 0,
)
for (i in 1:20) {
  sim_data <- mvrnorm(n=12, mu=rep(0,100), Sigma = Sigma_hat) |> t()
  Sigma_sim <- cov(sim_data |> t())

  # collate eigenvalues of sample covariance matrix of the simulated data
  temp[[length(temp) + 1]] <- tibble(
    number = 1:11,
    eigenvalue = eigen(Sigma_sim)$values[1:11],
    type = "simulated",
    iter = i,
  )
}
eigenvalue_data <- bind_rows(temp)

# Plot the eigenvalues
ggplot(eigenvalue_data |> filter(type != "reference"), aes(x=number, y = eigenvalue, color = type, group=iter)) +
  geom_line(linewidth=2, alpha=0.5) +
  geom_line(data = eigenvalue_data |> filter(type == "reference"), linewidth=4) +
  labs(x = "eigenvalue number", y = "eigenvalue")
```


One reason this occurs is that $\widehat \Sigma$ is low rank, specifically at most rank $n - 1$, which is less than $p$ in typical omics studies.
This means that any data generated using $\Sigma_{sim} = \widehat \Sigma$ will lie on a $n-1$-dimensional hyperplane.
Therefore, newly simulated samples will actually be linear combination of the samples of the reference data set.
Intuitively, this means that the entire probability distribution of possible $p$ dimensional data have been squished into a $n-1$ dimensional space, generating much higher gene-gene correlations than in the reference data set.
In contrast, we know that omics contains at least some level of random noise that is approximately independent from one gene to the next.
For example, RNA-seq involves a random sampling of fragments from the true sample of RNA molecules and other omics modes involve various background noise effects.
We therefore need to "inject" some level of additional randomness that is independent between genes on top of the correlations captured by $\widehat \Sigma$

We present three methods - called here PCA, spiked Wishart, and corpcor - for making this choice of $\Sigma_{sim}$, all which take the form
$$ \Sigma_{sim} = D^2 + UWU^T $$
where $D$ is a diagonal matrix capturing the injected independence and $UWU^T$ is a low-rank correlation matrix derived from $\widehat \Sigma$.
An additional advantage of this special form is that data may be generated very efficiently, Supplemental Figure S3 and see Methods, compared to arbitrary $\Sigma_{sim}$ which typically require a Cholesky or eigenvalue decomposition.

### Multivariate normal distribution

We first discuss the simplest case, where our data set is multivariate normally distributed.
The distribution $N(\mu, \Sigma)$ is the multivariate normal distribution with mean vector $\mu$ and covariance matrix $\Sigma$.
Here $\mu$ is a $p \times 1$ column vector and $\Sigma$ is a $p \times p$ matrix.
In order for this to work, $\Sigma$ must be symmetric and positive semi-definite, meaning that all of its eigenvalues are non-negative. 
This matrix is conceptually simple, since $\Sigma_{ij}$ gives the the covariance of $x_i$ and $x_j$ when $x \sim N(\mu, \Sigma)$.
More specifically, this is the population covariance matrix of $N(\mu, \Sigma)$, which does not generally equal the sample covariance matrix.
Indeed, if $X$ is $n$ samples from $N(\mu, \Sigma)$, then $\hat \Sigma := (X - \bar X) (X - \bar X)^T / (n - 1)$ is the sample covariance matrix where $\bar X$ is the average of the $n$ samples.
For large $n$, $\hat \Sigma$ will closely approximate $\Sigma$, but we care primarily about the situation where $n$ is small.
In particular, $\hat \Sigma$ is at most a rank $n - 1$ matrix while $\Sigma$ could be up to rank $p$.
This means that $N(\mu, \hat \Sigma)$ and $N(\mu, \Sigma)$ are quite different distributions: every sample from $N(\mu, \hat \Sigma)$ is contained in an $n-1$ dimensional plane.
If $n$ is small, then this is very unlike real data, which typically is close to $p$ dimensional.

The difficulty then is that we only know $\hat \Sigma$ from our reference data set, but we need to choose a $\Sigma$ with which we can simulate data and the obvious choice of $\hat \Sigma$ is inadequate.
The most common choice is to assume $\Sigma$ is a diagonal matrix.
This is the situation we want to avoid where the generated data is independent: $x_i$ and $x_j$ have zero covariance unless $i = j$.
However, this has some nice properties, such as being simple and fast to simulate (just generate univariate normal data for each variable).

We describe three alternative approaches in Methods of how to choose a $\Sigma$ that will produce data similar to the input data.
All three of these rely on a specific form of $\Sigma$, namely that
$$ \Sigma = D^2 + PP^T $$
where $D$ is a $p \times p$ diagonal matrix and $P$ is $p \times k$ for some $k \ll p$.
This is a combination of an independent part (the diagonal matrix) and a low-rank part ($P P^T$).
The low-rank part is also simple to generate data for.
If $x$ is a vector of $k$ independent univariate standard normal values, then $P x \sim N(0, P P^T)$.
The choice of specific $D$ and $P$ matrices is more in-depth and we leave the details for the Methods section.
We have three alternative means for doing so, which we refer to as the PCA, spiked Wishart, and corpcor methods.

Lastly, we emphasize that multivariate normal distributions do not capture all, or even most, types of possible dependence.
Indeed, we see this even in the 2-dimensional case where it is well known that correlation describes only a linear relationship between two variables while in reality they may have much more complex relations.
In higher dimensions, the problem is only worse.
So any method based off multivariate normal distributions are making large assumptions about distribution.
However, it is necessary to make some assumption like this.
In the next section, though, we see that "normal" part is actually not a large obstacle. 

### Gaussian copula

Building on the multivariate normal distribution, a popular approach to describe dependence in a high-dimensional settings is called the Gaussian copula approach.
The idea of this approach is that by normalizing and later reversing the normalization, data that does not fit a normal distribution can still have its dependence structure described using a multivariate normal distribution.
This allows the marginal (i.e., univariate) distributions of each genes to be specified separately from the dependence between genes.
This operates first by normalizing each gene by fitting a distribution (such as a normal distribution, Poisson, negative binomial, or other form), and then applying the fit cumulative distribution function (CDF) to the observed values.
Finally, those are fed to a standard normal distribution's inverse CDF to obtain values that are approximately normally distributed.
These values are then used to compute a covariance matrix $\Sigma$ and the data is assumed to follow a multivariate normal distribution in $p$ dimensions with that covariance matrix.

Here, we describe the approach using the form of covariance matrix $\Sigma = D^2 + PP^T$ as above.
Once data is obtained $Z \sim N(0, \Sigma)$, then one can undo the normalization process to obtain data with the same marginal distributions as the fit marginal distributions but with dependence determined by $\Sigma$.
We describe this in detail:

1.  Fit marginal distributions to each feature in $X$ to determine CDFs $F_{i}$ for each feature.
2.  Transform $X$ to normalized values by $Z_{ij} = \Phi^{-1}(F_{i}(X_{ij}))$ where $\Phi$ is the CDF of the standard normal distribution.
3.  Compute $D$, $U$, $W$ matrices from $X$ by one of three methods (see Methods).
4.  Generate $k$ i.i.d. standard normally distributed values $u$ and $p$ i.i.d standard normally distributed values $v$.
5.  Set $Z' = UWu + D v$.
6.  Output the vector $X'$ where $X'_i = F_i^{-1}(\Phi(Z'))$.

The generated data $Z'$ has covariance matrix $\Sigma = D^2 + U W U^T = D^2 + P P^T$, where $P := U \sqrt{W}$.
Moreover, we require that $\Sigma$ satisfies that $e_i \Sigma e_i^T$ is approximately 1.
That guarantees that the output $X'$ has each entry with the same marginal distributions $F_i$ as was originally fit and inherits gene-gene dependence from $Z'$.
This method is computationally efficient, taking hardly any more time or memory than simulations without dependence.

### Comparison to real data

To compare the three simulation methods with a real data set, we chose as a references data set 12 mouse cortex RNA-seq samples from accession GSE151923 [@Wang2022].
We then simulated data mimicking this reference using all three simulation methods (PCA, spiked Wishart, and corpcor) as well as a simulation with independent genes.
We repeated the simulations, each of 12 samples, a total of 8 times to estimate variance.
For the PCA method, we used $k=2$ dimensions and for the spiked Wishart, $k=11$.
The coprcor method always uses the full data matrix, analogous to $k=11$.
Note that PCA method must use a rank $k<11$ in order to generate full-rank data, see Methods, so these parameters are not directly comparable across methods.

We also ran for comparison SPsimSeq, another RNA-seq simulator for both bulk and single-cell that uses a Gaussian copula-based model of dependence, see Supplemental Methods for details.

Simulated data captures the genes' mean and variance accurately (@fig-compare-to-real a-b).
Next, we compared to the real data set when projected onto the top two principal components of the real data set (@fig-compare-to-real c).
The simulations with dependence are distributed around the entire space like the real data, but the independent simulations have unrealistically low variance in these components, clustering tightly around the origin.

Then, we computed the gene-gene correlation on pairs of high-expressed genes (at least 100 mean reads in the real data set).
The simulation with independence showed the least levels of gene-gene correlations (@fig-compare-to-real d).
However, the PCA method overshot the reference data set and the spiked Wishart and corpcor methods only slightly improved upon the independent simulation.
The SPsimSeq simulator performed similarly to the PCA method.

Lastly, we compared the variances of principal components on each data set (@fig-compare-to-real e).
These were computed separately for each data set, unlike (@fig-compare-to-real c) which used the reference data set's PCA weights for all data sets.
The independent data has much lower variance than the real data set in the top four principal components.
The spiked Wishart method comes closest to the real data set, as it optimizes for fitting these values.
Surprisingly, the corpcor method performs only somewhat better than the independent method.
The PCA method puts a large amount of variance into the first two components (due to using $k=2$) and then undershoots the other components.
Like PCA, the SPsimSeq simulator, overshot the first PCA component but did a better job matching lower PCA coordinates.


```{r}
#| label: fig-compare-to-real
#| fig-cap: "Comparison to real data run on a mouse cortex data set from GSE151923. (a-b) Comparison of gene (a) mean expression and (b) variance, log-scaled in real and PCA simulated data. The line of equality is marked in black. Points are colored according to the density of points in their region. Wishart and corpcor methods give similar results (not shown). (c) Quantile-quantile plot comparing correlation values of gene pairs from real data and simulated data (both with and without dependence). Genes with at least 100 reads were used. Values on the diagonal line indicate a match between the simulated and real data sets. (d) Projections onto the top two principal components of the real data set for both real and simulated data. All 8 simulations (96 samples for each simulation) shown. (e) Principal component analysis was performed on all data sets and the variance captured by the top components is shown. Unlike (d), these components were fit from each data set considered separately instead of reusing the weights from the real data."
#| fig-width: 10
#| fig-height: 10
readRDS("../processed/compare_to_real_plot.GSE151923.RDS")
```

### DESeq2 application

We benchmarked DESeq2 [@Love2014], a popular differential expression analysis tool, using data sets simulated with dependence and ones simulated without dependence to compare its performances on both.
DESeq2 presents an interesting case because several aspects of it assume independence of genes and so may be adversely affected by gene-gene dependence.
First, the independent filtering step [@Bourgon2010-uv] assumes independence but has been reported to be robust to typical gene-gene dependence.
Relatedly, the false discovery rate (FDR) [@BH_FDR] allows only certain forms of dependence.
Lastly, DESeq2's empirical Bayes steps could possibly be affected by gene dependence.

We used a fly whole body RNA-Seq data set GSE81142, see Supplemental Figure 1, and selected samples of male flies without treatment and after at least 2 hours of feeding to simulate 5 "control" samples.
Unnormalized read counts were used as the reference data set and all simulations used the DESeq2-based negative binomial model.
We then randomly selected 5% of the genes to be differentially expressed, with absolute $\log_2$ fold change uniformly distributed between $0.2$ and $2.0$, either up or down regulated chosen randomly, and simulated 5 "experimental" samples.
This was repeated 20 times for each of four dependence conditions (independent, PCA, Wishart, and corpcor).

Finally, we ran DESeq2 on each simulated 5 vs 5 experiment and compared the output FDR with the true percentages of genes that are differential expressed (@fig-DESeq2 a-d).
We observed that DESeq2 is anti-conservative on all data sets, with similar mean true FDRs for each estimated FDR cutoff.
However, we note that this anti-conservative nature is most present at very low FDR thresholds, was typically associated with only a single digit number of false positive genes.
While that mean behavior was consistant whether or not the simulation included gene-gene dependence, simulations with dependence showed a substantially greater variance in the performance of DESeq2.

To demonstrate the application of our simulation method for another organism, we also simulated data sets using the mouse cortex data set GSE151923 [@Wang2022] and selected samples from male mice.
We then simulated differential expression experiments as above and observed a similar result (@fig-DESeq2 e-f) as for the fly whole body data sets.

```{r}
#| include: false
source("DE_plots.R")
```

```{r}
#| label: fig-DESeq2
#| fig-cap: "Performance of DESeq2 on simulated datasets. (a-d) Comparison of true false discovery proportions and DESeq2 reported False Discovery Rates, plotted on a log scale, for data sets simulated from the fly whole body data set (GSE81142), (a) without dependence, (b) using PCA, (c) using Wishart and (d) using corpcor.  Diagonal line represents perfect estimation of FDR. (e-h) Comparison of true false discovery proportions and DESeq2 reported FDR for data sets simulated from the mouse cortex data set (GSE151923), (e) without dependence, (f) using PCA, (g) using Wishart and (h) using corpcor."
#| fig-width: 12
#| fig-height: 6
DESeq2_fdr_plot
```

### CYCLOPS application

We next used our simulation method to benchmark CYCLOPS [@Anafi2017], which infers relative times for a set of unlabeled samples using an autoencoder to identify circular structures.
We chose a mouse cortex time series data set GSE151565, see Supplemental Figure 2, which contains a total of 77 samples every 3 hours, for 36 hours.
As before, unnormalized read counts were used as the reference data set and all simulations used the DESeq2-based negative binomial model.
We computed the dependence structure of the genes as well as the variances of marginal distributions using the 11 time point 0 samples after removing one outlier and computed the means of gene expressions at each time point.
We then used these to simulate 20 time series data sets for each of the independent, PCA, Wishart, and corpcor simulation methods.

We ran CYCLOPS on each data set with a list of cyclic mouse genes (from [@Zhang2014], JTK p-value $<0.05$), which yielded an estimated relative time for each sample.
We evaluated CYCLOPS' performance compared to true circadian time using the circular correlation [@correlation_coeff], defined as follows:
$$\rho = \frac{\sum_{1\leq i<j\leq n}\sin(X_i-X_j) \sin(Y_i-Y_j)}{(\sum_{1\leq i<j\leq n} \sin(X_i-X_j)^2)^{1/2}(\sum_{1\leq i<j\leq n} \sin(Y_i-Y_j)^2)^{1/2}},$$
where $n$ is the number of samples, $X_i$ and $Y_i$ are the true time and CYCLOPS-estimated time, respectively, for the $i$-th sample.
$\rho$ has value between $-1$ and $1$, and a $|\rho|$ close to $1$ indicates accurate predictions by CYCLOPS.

By default, CYCLOPS performs dimension reduction so that each dimension (called an "eigengene") contains at least 3% of the total variance.
We found that CYCLOPS performance depended significantly on this parameter, with the default producing good performance across all simulations.
However, when dropping CYCLOPS to require just 2% variance in each eigengene, we found that its performance depends significantly on the dependence structure of the simulated time series data (@fig-CYCLOPS a).
At that setting, CYCLOPS performance is much higher in the PCA method and moderately improved in Wishart method, compared to the independent method. 
This difference is likely driven by the difference in the number of eigengenes used (@fig-CYCLOPS b), which is a measure of how much dependence is present in the data set.
This demonstrates that the correlational structure of the transcriptome can have a major impact on performance.

```{r}
#| include: false
source("circular_correlation.R")
```

```{r}
#| label: fig-CYCLOPS
#| fig-cap: "Performance of CYCLOPS on simulated time series data sets based on mouse cortex data set (GSE151565), using eigengenes of at least 2% variance. (a) Absolute circular correlations between true phases and CYCLOPS estimated phases on the simulated data sets. (b) The number of eigengenes used by CYCLOPS; the dotted line indicates the number of eigengenes used by CYCLOPS on the real data (5). (c-f) Examples of CYCLOPS estimated phases on the simulated data sets. CYCLOPS shows good performance when it separates out points by color (true circadian time)."
#| fig-width: 12
#| fig-height: 8
cyclops_plot
```

## Methods

Below, we describe the three methods for selecting the components of the covariance matrix $\Sigma_{sim} := D^2 + P P^T$.

###  PCA method

The first of our three methods attempts to match the top $k$ PCA components of the normalized reference dataset $Z$.
Specifically, let $u_1, \ldots, u_k$ be the left singular vectors of $Z$ with $\lambda_1, \ldots, \lambda_k$ the corresponding top $k$ signular values.
This method computes $\Sigma_{sim}$ such that $u_i^T \Sigma_{sim} u_i = \lambda_i^2$, i.e. that the variance in the direction of $u_i$ exactly matches of the reference dataset's variance in that same direction.
One solution is to use the sample covariance matrix, but that is not full rank and would match for all $i \leq n$ instead of just $i \leq k$.
Instead, we use the following:

1. Compute $A_{ij} = \delta_{ij} - \sum_\ell U_{\ell,i}^2 U_{\ell,j}^2$ and $B_{i} = \lambda_i^2/(n-1)^2 - \sum_\ell U_{\ell,i}^2 V_{\ell\ell}$ where $\delta_{ij}$ is the Kronecker delta and $V = Z^T Z / (n-1)$ is the covariance matrix of $Z$.
2. Solve $A w = B$ and set $W$ to be the diagonal matrix with $w$ along its diagonal.
3. Set $U$ to be the $p \times k$ matrix with columns $u_i$.
4. Set $D$ to be the diagonal matrix with $D_{ii} = \sqrt{V_{ii} - (UW^2U^T)_{ii}}$, which is the remaining variance.

Steps 1 and 2 give that $u_i \Sigma_{sim} u_i^T = \lambda_i$ for $i = 1, \ldots, k$.
Step 4 ensures that $e_j \Sigma_{sim} e_j^T = 1$ for $j = 1, \ldots, p$.

### Spiked Wishart method

The second method also makes use of PCA but has a different objective.
If $n$ samples are drawn from $N(0, \Sigma_{sim})$ then we want the variances of their PCA components to match those of the reference dataset.
Specifically, let $\lambda_1, \ldots, \lambda_{n-1}$ be the ${n-1}$ non-zero singular values of $Z$ ($\lambda_n$ is always approximately zero due to the normalization procedure), and let $\lambda'_1, \ldots, \lambda'_{n-1}$ be the singular values of $Z'$ where $Z'$ has $n-1$ columns each iid $N(0, \Sigma_{sim})$.
Then we want to choose $\Sigma_{sim}$ such that $E[\lambda'_i] = \lambda_i$ for each $i$, where $E[Y]$ denotes the expectation of the random variable $Y$.

Since the distribution of the $\lambda'_i$ does not have a known analytic solution, we approximate this situation with the spiked Wishart distribution.
The rank $k-1$ Wishart distribution is that of the sample covariance matrix of $Y$ where the $n$ columns of $Y$ are iid $N(0, \Sigma_{sim})$.
The spiked Wishart is the special case where $\Sigma_{sim}$ has $k$ arbitrary eigenvalues and the remaining are all equal to a constant.
Note that the singular values of $Y$ are the square roots of the eigenvalues of its sample covariance matrix.
While our case has $\Sigma_{sim}$ non-diagonal, $\Sigma_{sim}$ may be diagonalized by orthogonal rotations due the sepctral theorem, and orthogonal rotations do not change the singular values of $Y$.
Therefore, the distribution of singular values is not affected by the assumption that $\Sigma_{sim}$ is diagonal.
Moreover, for the form $\Sigma_{sim} = D^2 + U W U^T$ where $p$ is very large, each column of $U$ is typically very close to orthogonal to any $e_i$, a standard basis vector.
Therefore, when $D = c^2I$ for some constant $c$, we can approximate $\Sigma_{sim}$ as having $k$ arbitrary eigenvalues from $W$ and $n$ remaining eigenvalues all equal to $c$ corresponding to $D$.
This is a spiked Wishart distribution.

However, the spiked Wishart distribution also has no known analytic solution for the distribution of its eigenvalues either.
Therefore, we use an efficient sampling and stochastic gradient descent method that we recently described [@wishart].
Since the dataset has been normalized, $c$ will be close to one and $e_i \Sigma_{sim} e_i^T \approx 1$.

Specifically, we do:

1. Set $U$ to be the $p \times k$ matrix with columns $u_i$, the left singular vectors of $Z$.
2. Compute $w_1, \ldots, w_k$ and $c$ by stochastic gradient descent minimizing $\sum_{i} (E[\lambda'_i] - \lambda_i)^2$ for $\Sigma_{sim}$ diagonal with entries $w_1, \ldots, w_k, c^2, \ldots, c^2$ [@wishart].
4. Set $W$ diagonal with the entries $w_1, \ldots, w_k$.
3. Set $D = cI$

### Corpcor method

The `corpcor` package [@SchaferStrimmer2005; @OpgenRheinStrimmer2007] computes a James-Stein type shrinkage estimator for the covariance matrix.
For large $p$, this greatly improves the estimate of the covariance matrix by introducing a little bias towards zero correlations and equal variances of genes.
It computes optimal values of $\lambda_1$, and $\lambda_2$, its two regularization coefficients.
It then uses $\lambda_1$ to linearly interpolate the sample covariance matrix towards the identity matrix $I$ and $\lambda_2$ to interpolate the vector of variances towards the median variance value.
Since the sample covariance matrix is rank at most $n$, we again obtain a matrix of the form $\Sigma_{sim} = D^2 + UWU^T$.

This algorithm is:

1. Compute the $\lambda_1$ and $\lambda_2$ values from `corpcor::estimate.lambda` and `corpcor::estimate.lambda.var` functions on $Z$, respectively.
2. Set $D$ to be diagonal with $D_{ii} = \sqrt{\lambda_1} (\lambda_2 \sigma_{med} + (1 - \lambda_2) \sigma_i)$ where $\sigma_i$ is the standard deviation of the $Z_{i\cdot}$ and $\sigma_{med}$ is the median of the $\sigma_i$.
3. Set $U$ to be $\sqrt{1-\lambda} S Z / \sqrt{n-1}$ where $S$ is the diagonal matrix with $S_{ii} = \lambda_2 \sigma_{med} / \sigma_i + (1 - \lambda_2)$.
4. Set $W$ to the identity.

## Discussion

We described the well-known Gaussian copula approach and recommended a specific form of covariance matrix which is well tailored to omics data simulation.
We developed three methods using this form of covariance matrix which can be used to mimic a reference data set for simulation.
All of these methods use a multivariate normal distribution as an intermediate step and therefore substantially restrict the kinds of dependence that can be simulated.
However, when operating in a high-dimensional space some simplification is likely required.

To encourage adoption of dependence in simulated omics data, we developed `dependentsimr`, an R package that generates omics-scale data with realistic correlation.
This implementation is efficient and simple, requiring just two lines of code to fit a model to a reference data set and then simulate data from it.
We demonstrated this package on RNA-seq data, using the DESeq2 method to fit negative binomial marginal distributions.
However, this package is actually quite general and supports normal, Poisson, negative binomial, and arbitrary ordered discrete distributions using the empirical CDF.
Moreover, it can support multi-modal data such as is increasingly common in multi-omics.

We demonstrated the importance of including gene-gene dependence in simulated data by two application benchmarks.
In the first, DESeq2 results were substantially more variable when simulating with gene-gene dependence.
This indicates that DESeq2's had an increased chance of a larger simultaneous number of false positives, as well as increased chance of fewer false positives, when data included realistic dependence.
We therefore recommend that benchmarking of differential expression methods should include gene-gene dependence whenever possible.
This is despite the fact that differential expression is largely addressing a problem of the behavior of individual genes.
In the second, CYCLOPS performance in estimating circadian phases depends upon gene-gene dependence, was sensitive to dependence structure of the data.
Unlike DESeq2, CYCLOPS explicitly makes use of the correlations of genes and therefore this result is not surprising but still demonstrates the potential impact of the assumption of independence when benchmarking.

Our comparisons to a real dataset show that none of our three methods are able to exactly capture all aspects of the real dataset.
In particular, the gene-gene correlations were too high in the PCA method and too low in the spiked Wishart and corpcor methods.
Surprisingly, the spiked Wishart and corpcor methods improved in this metric only slightly compared to the simulations with independent genes.
These observations demonstrate that there is room for future improvements over independent data in these techniques, possibly incorporating more recent developments in copulae [@copulae].
Possibly, this could demonstrate the limitations of methods based on the multivariate normal distribution or of the low-rank approximation used by all three of our methods. 
Nonetheless, these methods represent significant improvements by other metrics and we recommend the inclusion of some dependence in nearly every simulated omics dataset.

### Alternatives

We highlight some alternative approaches and software packages that have been taken below.

The R package [SPsimSeq](https://github.com/CenterForStatistics-UGent/SPsimSeq) [@Assefa2020] provides a dedicated RNA-seq and single-cell RNA-seq simulator using a Gaussian copula approach to simulate gene dependence.
In contrast to this package, it uses a two-step randomization.
In short, it first fits distributions to each genes and then bins those distributions into discrete buckets.
Then, new data is generated by drawing from a multivariate normal distribution whose covariance matrix equals the sample covariance matrix of the reference data set.
These values are used to choose which bucket each gene is drawn from.
Then, each gene's final value is drawn uniformly and independently from within its chosen bucket.
Therefore, SPsimSeq induces correlation in the first multivarainte normal draw, but then injects additional independence in the second uniform step.
This two-step process makes it challenging to compare on theoretical grounds to our proposed methods since it will dependend upon parameters such as the number of buckets used.
More fine-grained buckets will induce higher correlation, and in the limit of infinite buckets, it should approach our PCA method with $k$ equal to the number of samples in the reference data set.
`SPsimSeq` is more specialized and full-featured for RNA-seq simulation, providing, for example, native differential expression (DE) options,.
In comparison, our `dependentsimr` package requires manually setting marginal expression values to inject DE, but also supports other marginal distributions for situations outside of RNA-seq.

The [scDesign2](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02367-2) simulator [@Sun2021] for single-cell RNA-seq also uses Gaussian copula and, like our method, uses the approach of estimating the correlation matrix from the normalized dataset.
However, it limits this correlation matrix to top-expressed genes.
Since correlation is most discernible in high-expressed genes, this approach is reasonable but requires making certain arbitrary cutoffs that our methods avoid.

Other Gaussian copula-based R packages that may be applicable, at least for datasets with smaller numbers of features, include `bindata`, `GenOrd`, and `SimMultiCorrData`, the last of these being the most comprehensive.
The `bigsimr` package provides faster implementations of these methods to scale up to omics-level data.
However, even this is computationally demanding; their paper references generating 20,000-dimensional vectors in “under an hour” using 16 threads.
The `copula` package provides even more flexible dependence options through use of copulas.
All of these packages provide more flexibility in specifying dependence than our package, which can only mimic existing datasets, and therefore the longer run-times may be unavoidable for use cases where researchers need to parameterize the dependence structure.

## Data availability

Source code for all simulations and figures in this plot is available at [github.com/itmat/dependent_sim_paper/](https://github.com/itmat/dependent_sim_paper/).
Source code for the `dependentsimr` package is available at [github.com/tgbrooks/dependent_sim](https://github.com/tgbrooks/dependent_sim/).
All data used is available from the Gene Expression Omnibus (GEO) with accession numbers GSE151923, GSE81142, GSE151565.

## Funding statement and competing interests

JY received funding from National Institute of Neurological Disorders and Stroke (5R01NS048471).
TB and GG received funding support from the National Center for Advancing Translational Sciences Grant (5UL1TR000003).
The funders had no role in this research, the decision to publish, or the preparation of this manuscript.

The authors declare no competing interests.
