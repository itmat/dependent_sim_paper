---
title: Generating Correlated Data for Omics Simulation
editor: 
  markdown: 
    wrap: sentence
execute:
  echo: false
  cache: true
format:
  html:
    toc: true
    toc-expand: true
bibliography: references.bib
---

## Abstract

Simulation of realistic omics data is a key input for benchmarking studies that help users obtain optimal computational pipelines.
Omics data involves large numbers of measured features on each samples and these measures are generally correlated with each other.
However, simulation too often ignores these correlations, perhaps due to the inconvenience and computational hurdles of doing so.
We describe and implement three approaches based off decomposing the covariance matrix into a diagonal part and a low-rank part and using a Gaussian copula approach.
These approaches are efficient at omics-scale problems.
We demonstrate the importance of including correlation in two benchmarking applications.
First, we show that variance of results from the popular DESeq2 method increases when dependence is included.
Second, we demonstrate that a method, CYCLOPS, for inferring circadian time of collection from transcriptomics actually improves its performance when dependence is included.
We provide an R package, dependentsimr, that has efficient implementations of these methods and can generate dependent data with arbitrary distributions, including discrete (binary, ordered categorical, Poisson, negative binomial), continuous (normal), or with an empirical distribution.

## Introduction

Omics data is in the $p \gg n$ regime where there are fewer samples than measurements per sample.
This creates dual challenges in generating realistic simulated data for the purposes of benchmarking.
First, there isn’t enough data to be able to compute a dependence structure (e.g., a full-rank correlation matrix).
Second, generating omics-scale data with a specified correlation matrix is slow due to the typical $O(p^3)$ nature of these algorithms.
These often mean that simulators assume independence of the measurements, which does not reflect reality.

Recently, we wrote guidelines for performing omics benchmarking [@Brooks2024] and encouraged authors to ensure that any simulated data includes realistic correlation and dependence of measurements within a sample.
However, this highlighted that one reason this is often neglected is that there is relatively little guidance for how to achieve it and that existing solutions are cumbersome.

Here, we give solutions to this problem by using a correlation matrix that decomposes into a diagonal part and a low-rank part to both approximate realistic dependencies in a real data set and generate simulated data mimicking the real data.
Using a Gaussian copula approach (also referred to as NORTA, for "normal to anything"), the marginal (univariate) distributions can have realistic forms.
These solutions operate by taking a real dataset and mimicking it.
We implement this strategy as an R package which supports normal, Poisson, DESeq2-based (negative binomial with sample-specific size factors), and empirical (for ordinal data) marginal distributions.
This makes it particularly suited to RNA-seq data but also widely applicable.

We implement three different strategies for determining the diagonal and low-rank parts of the covariance matrix.
First, the 'PCA' method uses principal component analysis (PCA) and pick the low-rank part such that the simulated data has the same variance in the top $k$ PCA components of the reference dataset.
Second, the 'spiked Wishart' method fits $k$ components such that simulations with the same number of samples as the reference dataset will have, on average, the same PCA component variances as the reference 
Unlike 'PCA', these variances are computed with resepect to the simulated data's own PCA and not using the PCA weights of the reference dataset.
Third, the 'corpcor' method uses the popular R library `corpcor`, which implements a James-Stein type shrinkage estimator for the covariance matrix as a linear interpolation of the sample covariance matrix and a diagonal matrix.
No method exactly captures the input data, indicating room for future research, but all improve upon a the common approach of assuming independence.

We show two applications which demonstrate the importance of including dependence of measurements in simulated data when benchmarking computational pipelines.
First, we simulate RNA-seq data with differential expression between two conditions.
Using DESeq2 to determine the differentially expressed genes, we found that dependence had little impact on the accuracy of reported p-values but increased the variance of those estimates, meaning that experiments simulated with dependence were more likely to be conservative and more likely to be anti-conservative compared to simulations of independent data.
Second, we simulated a time series of RNA-seq data points and used the CYCLOPS method to infer collection time from the RNA-seq data, without time labels.
Surprisingly, we found that including dependence in the data set resulted in improved performance with CYCLOPS.

## Results

### Comparison to Real Data

```{r}
#| include: false
source("compare_to_real.R")
```

<!-- TODO which dataset do we want to use here? -->
We used mouse liver RNA-seq data set from accession GSE77221 [@Weger2019] where we expected a significant amount of gene-gene correlation due to the samples having been collected throughout the course of a day.
We used this data to prime our simulation and compared real and simulated data sets, both with 12 samples, to each other.
We repeated the simulations a total of 8 times to estimate variance.
The simulations were then repeated without any dependence for comparison.

Simulated data captures the genes' mean and variance accurately (@fig-compare-to-real a-b).
Moreover, we computed the gene-gene correlation on pairs of high-expressed genes.
Since only 12 samples are in each real or simulated data set, correlation estimates are very noisy.
However, the distribution of correlations across all pairs of these genes is much closer to that of real data when simulated with dependence than without (@fig-compare-to-real c).

Next, we compared the simulations with and without dependence to the real data set when projected onto the top two principal components of the real data set (@fig-compare-to-real d).
The simulations with dependence are distributed around the entire space like the real data, but the independent simulations have unrealistically low variance in these components, clustering tightly around the origin.
Lastly, we compared the PCA of each simulated data set considered separately from the real data.
Simulations with independent data showed unrealistically low levels of variance in top principal components, but dependent simulations actually show too high elevations of variance in top components.
This is a consequence of our PCA strategy: since variance in the direction of the real data set's top principal component is exactly matched, the simulated data set's top principal component will be even larger than that.

```{r}
#| label: fig-compare-to-real
#| fig-cap: !expr glue::glue("Comparison to real data run on a mouse liver example from GSE77221. (a-b) Comparison of gene (a) mean expression and (b) variance, log-scaled. The line of equality is marked in black. Points are colored according to the density of points in their region. (c) Quantile-quantile plot comparing correlation values of gene pairs from real data and simulated data (both with and without dependence). Genes with at least {HIGH_EXPR_CUTOFF} reads were used. Values on the diagonal line indicate a match between the simulated and real data sets. (d) Projections onto the top two principal components of the real data set for both real and simulated data. All 8 simulations (96 samples total for each of dependent and independent simulations) shown. (e) Principal component analysis was performed on all data sets and the variance captured by the top components is shown. Unlike (d), these components were fit from each data set considered separately instead of reusing the weights from the real data.")
#| fig-width: 10
#| fig-height: 10
compare_to_real_plot
```

### DESeq2 Application

We benchmark DESeq2 [@Love2014], a popular differential expression analysis tool, using data sets simulated with dependence and ones simulated without dependence to compare its performances on both. We use a fly whole body RNA-Seq data set GSE81142 and select samples of male flies without treatment and after at least 2 hours of feeding to simulate 20 control samples with rank $k=2$ dependence structure, and 20 control samples assuming independence of the genes. We then randomly select 5% of the genes to be up or down regulated, with $\log_2$ fold change uniformly distributed between $0.2$ and $2.0$, and simulate 20 "experimental" samples each with and and without dependence.

Finally, we run DESeq2 on the two simulated experiments and compare the output false discovery rate (FDR) with the true percentages of genes that are differential expressed (@fig-DESeq2 a-b). We observe that DESeq2 is slightly anti-conservative on both data sets, with similar mean true FDRs for each estimated FDR cutoff. However, there is a greater variance in the performance of DESeq2 on the data sets simulated with dependence, indicating that it potentially performs less reliably on data sets with gene-gene dependence, as in real data sets. We speculate that this is due to DESeq2's assumption that the gene expressions are independent.

To demonstrate the application of our simulation method for another organism, we also simulate data sets using a mouse cortex data set GSE151923 and select samples from male mice to infer the dependence structure of the quantified genes. We then simulate differential expression experiments with and without dependence as above, with 20 samples for each condition. We observe a similar result (@fig-DESeq2 c-d) as for the fly whole body data sets.

```{r}
#| include: false
source("DE_plots.R")
```

```{r}
#| label: fig-DESeq2
#| fig-cap: "Performance of DESeq2 on simulated datasets. (a-b) Comparison of true FDR and DESeq2 reported FDR for data sets simulated from the fly whole body data set (GSE81142), (a) without dependence and (b) with dependence. (c-d) Comparison of true FDR and DESeq2 reported FDR for data sets simulated from the mouse cortex data set (GSE151923, Wang et al. 2022), (c) without dependence and (d) with dependence."
#| fig-width: 7
#| fig-height: 6
DESeq2_fdr_plot
```

### CYCLOPS Application

We further use our simulation method to benchmark CYCLOPS [@Anafi2017], which infer relative times for a set of unlabeled samples using an autoencoder to identify circular structures. We use a mouse liver time series data set (GSE151565, Aaronson et al. 2020), which contains a total of 77 samples every 3 hours, for 36 hours. We compute the dependence structure of the genes as well as the variances of marginal distributions using the 12 time point 0 samples, and compute the means of gene expressions at each time point. We then use these to simulate 20 time series data sets with rank $2$ dependence structure, each with 8 time points and 6 samples per time point, and 20 such data sets without dependence.

We run CYCLOPS on each data set with a list of cyclic mouse liver genes (JTK p-value $<0.05$), which yields an estimated relative time for each sample. For the evaluation metric for CYCLOPS' performance, we use the circular correlation, defined as follows:
$$\rho = \frac{\sum_{1\leq i<j\leq n}sin(X_i-X_j)sin(Y_i-Y_j)}{(\sum_{1\leq i<j\leq n}sin(X_i-X_j)^2)^{1/2}(\sum_{1\leq i<j\leq n}sin(Y_i-Y_j)^2)^{1/2}},$$
where $n$ is the number of samples, $X_i$ and $Y_i$ are the true time and CYCLOPS-estimated time respectively for the $i$-th sample. $\rho$ has value between $-1$ and $1$, and a $|\rho|$ close to $1$ indicates accurate predictions by CYCLOPS.

We find that CYCLOPS performs better overall on simulated time series data sets with dependence than on those without dependence (@fig-CYCLOPS), indicating that the presence of gene-gene dependence is essential for CYCLOPS' performance.

```{r}
#| include: false
source("circular_correlation.R")
```

```{r}
#| label: fig-CYCLOPS
#| fig-cap: "Performance of CYCLOPS on simulated time series data sets based on mouse liver data set (GSE151565, Aaronson et al. 2020). (a) Violin plot comparing absolute circular correlations between true phases and CYCLOPS estimated phases on the simulated data sets with and without dependence. (b-c) Examples of CYCLOPS estimated phases on a simulated data set (b) without dependence and (c) with dependence."
cyclops_plot
```

## Methods

Assume that we have a reference dataset $X$ given by an $p \times n$ data matrix of $p$ features measured in each of $n$ independent samples.
We want to capture correlations between the $p$ features, which could represent gene expressions, protein abundances, or other measured values.
We refer to these features as genes for simplicity.
Our goal is to generate simulated data with the same $p$ genes, the same marginal distributions of each gene as in $X$ and realistic gene-gene dependence.


### Gaussian Copula

All three methods are based off a well-known Gaussian copula approach.
This operates first by normalizing each gene by fitting a distribution (such as a normal distribution, Poisson, negative binomial, or other form), and then applying the fit cumulative distribution function (CDF) to the observed values.
Finally, those are fed to a standard normal distribution's inverse CDF to obtain values that are approximately normally distributed.
These values are then used to compute a covariance matrix $\Sigma$ and the data is assumed to follow a multivariate normal distribution in $p$ dimensions with that covariance matrix.
Note that the sample covariance matrix is a poor choice for this $\Sigma$.
In general, the sample covariance matrix is at most rank $n$, and will be approximately rank $n-1$ due to the normalization procedure.
This means that generated data would lie on an $n-1$ dimensional hyperplane, which we know is unrealistic.

Instead, we want to estimate some other matrix of the form $\Sigma = D + PP^T$ where $D$ is $p \times p$ diagonal and $P$ is $p \times k$ for $k \ll p$.
This means that $P P^T$ is low-rank.
Conveniently, this form allows efficient simulation of multivariate normal data $N(0, \Sigma)$.
Once data is obtained $Z ~ N(0, \Sigma)$, then undo the normalization process to obtain data with the same marginal distributions as the fit marginal distributions but with dependence determined by $\Sigma$.
We describe this in detail below:

1.  Fit marginal distributions to each feature in $X$ to determine CDFs $F_{i}$ for each feature.
2.  Transform $X$ to normalized values by $Z_{ij} = \Phi^{-1}(F_{i}(X_{ij}))$ where $\Phi$ is the CDF of the standard normal distribution.
3.  Compute $D$, $U$, $W$ matrices from $X$ by one of three methods (below).
4.  Generate $k$ i.i.d. standard normally distributed values $u$ and $p$ i.i.d standard normally distributed values $v$.
5.  Set $Z' = UWu + D v$.
6.  Output the vector $X'$ where $X'_i = F_i^{-1}(\Phi(Z'))$.

The generated data $Z'$ has covaraince matrix $\Sigma = D + U W U^T$.
Moreover, we require that $\Sigma$ satisfies that $e_i \Sigma e_i^T$ is approximately 1.
That guarantees that the output $X'$ has each entry with the same mariginal distributions $F_i$ as was originally fit and inherits gene-gene dependence from $Z'$.

This is a standard Gaussian copula approach tailored to our specific format of $\Sigma$.
Most implementations of this approach instead rely upon performing a Cholesky decomposition of $\Sigma$, which requires significant time and memory resources for omics-scale problems with over tens of thousands of genes.

###  PCA Method

The first of our three methods attempts to match the top $k$ PCA components of the normalized reference dataset $Z$.
Specifically, let $u_1, \ldots, u_k$ be the left singular vectors of $Z$ with $\lambda_1, \ldots, \lambda_k$ the corresponding top $k$ signular values.
This method computes $\Sigma$ such that $u_i^T \Sigma u_i = \lambda_i^2$, i.e. that the variance in the direction of $u_i$ exactly matches of the reference dataset's variance in that same direction.
One solution is to use the sample covariance matrix, but that is not full rank and would match for all $i \leq n$ instead of just $i \leq k$.
Instead, we use the following:

1. Compute $A_{ij} = \delta_{ij} - \sum_\ell U_{\ell,i}^2 U_{\ell,j}^2$ and $B_{i} = \lambda_i^2/(n-1)^2 - \sum_\ell U_{\ell,i}^2 V_{\ell\ell}$ where $\delta_{ij}$ is the Kronecker delta and $V = Z^T Z / (n-1)$ is the covariance matrix of $Z$.
2. Solve $A w = B$ and set $W$ to be the diagonal matrix with $w$ along its diagonal.
3. Set $U$ to be the $p \times k$ matrix with columns $u_i$.
4. Set $D$ to be the diagonal matrix with $D_{ii} = \sqrt{V_{ii} - (UW^2U^T)_{ii}}$, which is the remaining variance.

Step 4 ensures that $e_i \Sigma e_i^T = 1$.

### Spiked Wishart Method


The second method also makes use of PCA but has a different objective.
If $n$ samples are drawn from $N(0, \Sigma)$ then we want the variances of their PCA components to match those of the reference dataset.
Specifically, let $\lambda_1, \ldots, \lambda_{n-1}$ are the ${n-1}$ non-zero singular values of $Z$ ($\lambda_n$ is always approximately zero due to the normalization procedure), and let $\lambda'_1, \ldots, \lambda'_{n-1}$ be the singular values of $Z'$ where $Z'$ has $n-1$ columns each iid $N(0, \Sigma)$.
Then we want to choose $\Sigma$ such that $E[\lambda'_i] = \lambda_i$ for each $i, where $E[Y]$ denotes the expectation of the random variable $Y$.

Since the distribution of the $\lambda'_i$ does not have a known analytic solution, we approximate this situation with the spiked Wishart distribution.
The rank $k-1$ Wishart distribution is that of the sample covariance matrix of $Y$ where the $n$ columns of $Y$ are iid $N(0, \Sigma)$.
The spiked Wishart is the special case where $\Sigma$ has $k$ arbitrary eigenvalues and the remaining are all equal to a constant.
Note that the singular values of $Y$ are the square roots of the eigenvalues of its sample covariance matrix.
While our case has $\Sigma$ non-diagonal, $\Sigma$ may be diagonalized by orthogonal rotations due the sepctral theorem, and orthogonal rotations do not change the singular values of $Y$.
Therefore, the distribution of singular values is not affected by the assumption that $\Sigma$ is diagonal.
Moreover, for the form $\Sigma = D + U W U^T$ where $p$ is very large, each column of $U$ is typically very close to orthogonal to any $e_i$, a standard basis vector.
Therefore, when $D = cI$ for some constant $c$, we can approximate $\Sigma$ as having $k$ arbitrary eigenvalues corresponding to $U$ and $n$ remaining eigenvalues all equal to $c$ corresponding to $D$.
This is a spiked Wishart distribution.

However, the spiked Wishart distribution also has no known analytic solution for the distribution of its eigenvalues either.
Therefore, we use a stochastic gradient descent method that we recently described. <!-- TODO cite -->
Since the dataset has been normalized, $c$ will be close to one and $e_i \Sigma e_i^T \approx 1$.

Specifically, we do:

1. Set $U$ to be the $p \times k$ matrix with columns $u_i$, the left singular vectors of $Z$.
2. Compute $w_1, \ldots, w_k$ and $c$ by stochastic gradient descent minimizing $\sum_{i} (E[\lambda'_i] - \lambda_i)^2$ for $\Sigma$ diagonal with entries $w_1, \ldots, w_k, c, \ldots, c$.
4. Set $W$ diagonal with the entries $w_1, \ldots, w_k$.
3. Set $D = cI$

### Corpcor Method

The `corpcor` package [@SchaferStrimmer2005; @OpgenRheinStrimmer2007] computes a James-Stein type shrinkage estimator for the covariance matrix.
For large $p$, this greatly improves the estimate of the covariance matrix by introducing a little bias towards zero correlations and equal variances of genes.
It computes optimal values of $\lambda_1$, and $\lambda_2$, its two regularization coefficients.
It then uses $\lambda_1$ to linearly interpolate the sample covariance matrix towards the identity matrix $I$ and $\lambda_2$ to interpolate the vector of variances towards the median variance value.
Since the sample covariance matrix is rank at most $n$, we again obtain a matrix of the form $\Sigma = D + UWU^T$.

This algorithm is:

1. Compute the $\lambda_1$ and $\lambda_2$ values from `corpcor::estimate.lambda` and `corpcor::estimate.lambda.var` functions on $Z$, respectively.
2. Set $D$ to be diagonal with $D_{ii} = \sqrt{\lambda_1} (\lambda_2 \sigma_{med} + (1 - \lambda_2) \sigma_i)$ where $\sigma_i$ is the standard deviation of the $Z_{i\cdot}$ and $\sigma_{med}$ is the median of the $\sigma_i$.
3. Set $U$ to be $\sqrt{1-\lambda} S Z / \sqrt{n-1}$ where $S$ is the diagonal matrix with $S_{ii} = \lambda_2 \sigma_{med} / \sigma_i + (1 - \lambda_2)$.
4. Set $W$ to the identity.

## Discussion

We present `dependentsimr`, an R package that generates omics-scale data with realistic correlation.
We demonstrate this package on RNA-seq data, using the DESeq2 method to fit negative binomial marginal distributions.
However, this package is actually quite general and supports normal, Poisson, negative binomial, and arbitrary ordered discrete distributions using the empirical CDF.
Moreover, it can support multi-modal data such as is increasingly common in multi-omics.

We demonstrate the importance of including gene-gene dependence in simulated data by two application benchmarks.
In the first, we demonstrate that DESeq2 results are substantially more variable when simulating with gene-gene dependence.
In the second, we demonstrate that the performance of CYCLOPS in estimating circadian phases depends upon gene-gene dependence, with an unexpected result of better performance in the dependent case.

Our comparisons to a real dataset show that none of our three methods are able to exactly capture some aspects of the real dataset.
In particular, the gene-gene correlations were too high in the PCA method and too low in the spiked Wishart and corpcor methods.
Surprisingly, spiked Wishart and corpcor methods improved in this metric only slightly compared to the simulations with independent genes.
These observations demonstrate that there is room for future improvements in these techniques and possibly demonstrate that the datasets are too far from a multivariate normal distribution even after normalization. 
Nonetheless, these methods represent significant improvements by other metrics and we recommend the inclusion of some dependence in nearly every simulated dataset.
We highlight some alternatives that have been taken below.

### Alternatives

The R package [SPsimSeq](https://github.com/CenterForStatistics-UGent/SPsimSeq) [@Assefa2020] provides a dedicated RNA-seq and scRNA-seq simulator using a NORTA approach to simulate gene dependence.
In contrast to this package, it uses WGCNA to determine the correlation matrix, which is a gene network approach.
However, this method takes significant computational resources.
Indeed, the `SPsimSeq` paper generated data for just 5000 genes based on a randomly sampled 5000 gene subset of the RNA-seq data and our attempts to use `SPsimSeq` to generate a full sample exhausted the memory of a 24GB computer.
In contrast, our method runs in seconds to generate a 40,000 gene samples on the same computer.
`SPsimSeq` is more specialized and full-featured for RNA-seq simulation, providing, for example, native differential expression (DE) options.
In contrast, our `dependentsimr` package requires manually setting marginal expression values to inject DE, but also supports other marginal distributions for situations outside of RNA-seq.

The [scDesign2](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02367-2) simulator [@Sun2021] for scRNA-seq uses NORTA and, like our method, uses the approach of estimating the correlation matrix from the normalized dataset.
However, it limits this correlation matrix to top-expressed genes.

Other Gaussian copula-based R packages that may be applicable, at least for datasets with smaller numbers of features, include `bindata`, `GenOrd`, and `SimMultiCorrData`, the last of these being the most comprehensive.
The `bigsimr` package provides faster implementations of these methods to scale up to omics-level data.
However, even this is computationally demanding; their paper references generating 20,000-dimensional vectors in “under an hour” using 16 threads.
The `copula` package provides even more flexible dependence options through use of copulas (the NORTA approach is equivalent to using Gaussian copulas).
All of these packages provide more flexibility in specifying dependence than our package, which can only mimic existing datasets, and therefore the longer run-times may be unavoidable for use cases where researchers need to parameterize the dependence structure.
